{"cells":[{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T00:44:11.802435Z","iopub.status.busy":"2024-03-05T00:44:11.801990Z","iopub.status.idle":"2024-03-05T00:44:24.103293Z","shell.execute_reply":"2024-03-05T00:44:24.102162Z","shell.execute_reply.started":"2024-03-05T00:44:11.802403Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: efficientnet_pytorch in /opt/conda/lib/python3.10/site-packages (0.7.1)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.1.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (2024.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n"]}],"source":["!pip install efficientnet_pytorch"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T01:10:03.844722Z","iopub.status.busy":"2024-03-05T01:10:03.844289Z","iopub.status.idle":"2024-03-05T01:10:03.853921Z","shell.execute_reply":"2024-03-05T01:10:03.853041Z","shell.execute_reply.started":"2024-03-05T01:10:03.844687Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Subset\n","def test_train_val_split(dataset, filepath=\"/kaggle/input/cub-200-2011/CUB_200_2011/train_test_split.txt\", train_size = 0.9):\n","    \n","    # Read the train_test_split file\n","    split_file = open(\"/kaggle/input/cub-200-2011/CUB_200_2011/train_test_split.txt\")\n","    flags = split_file.readlines()\n","    N = len(flags)\n","    i = 0\n","    flags = [int(flags[i].split('\\n')[0].split(\" \")[1]) for i in range(N)]\n","    \n","    # Store the indices of training and testing data\n","    index1 = []\n","    index0 = []\n","\n","    for i in range(N):\n","        if(flags[i]==0):\n","            index0.append(i)\n","        else:\n","            index1.append(i)\n","    \n","    # Split the dataset based on the train_test split\n","    test_dataset = Subset(dataset, index0)\n","    train_val_dataset = Subset(dataset, index1)\n","    \n","    # Split the training dataset into training and validation set\n","    train_count = int(train_size * len(train_val_dataset))\n","    val_count  = len(train_val_dataset) - train_count\n","\n","    seed = torch.Generator().manual_seed(42)\n","    train_dataset, val_dataset = random_split(train_val_dataset, [train_count, val_count], generator=seed)\n","    \n","    return train_dataset, val_dataset, test_dataset"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T01:10:07.113296Z","iopub.status.busy":"2024-03-05T01:10:07.112752Z","iopub.status.idle":"2024-03-05T01:10:07.122805Z","shell.execute_reply":"2024-03-05T01:10:07.121749Z","shell.execute_reply.started":"2024-03-05T01:10:07.113258Z"},"trusted":true},"outputs":[],"source":["# Testing loop\n","def test(model, test_loader, device):\n","    model.eval()\n","    test_correct = 0\n","    test_total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)  # Move data to CUDA if available\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            _, predicted = torch.max(outputs, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","    test_accuracy = 100 * test_correct / test_total\n","    print(f'Testing Accuracy: {test_accuracy:.2f}%, Epochs: {epoch}')\n","    f.write(f'Testing Accuracy: {test_accuracy:.2f}%, Epochs: {epoch}\\n')"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T01:10:13.482409Z","iopub.status.busy":"2024-03-05T01:10:13.481733Z","iopub.status.idle":"2024-03-05T01:10:13.487797Z","shell.execute_reply":"2024-03-05T01:10:13.486738Z","shell.execute_reply.started":"2024-03-05T01:10:13.482377Z"},"trusted":true},"outputs":[],"source":["# Import important libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from efficientnet_pytorch import EfficientNet\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim.lr_scheduler import StepLR\n","import copy"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-03-05T01:10:16.668854Z","iopub.status.busy":"2024-03-05T01:10:16.668311Z","iopub.status.idle":"2024-03-05T01:36:08.482902Z","shell.execute_reply":"2024-03-05T01:36:08.481846Z","shell.execute_reply.started":"2024-03-05T01:10:16.668823Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded pretrained weights for efficientnet-b2\n","\n","Total number of parameters in the model: 7982794\n","\n","Total number of trainable parameters in the model: 2229136\n","\n","Training Epoch - [1/20], Batch [17/85], Loss: 5.2893\n","Training Epoch - [1/20], Batch [34/85], Loss: 5.2877\n","Training Epoch - [1/20], Batch [51/85], Loss: 5.2724\n","Training Epoch - [1/20], Batch [68/85], Loss: 5.2380\n","Training Epoch - [1/20], Batch [85/85], Loss: 5.2276\n","Epoch Loss: 5.2630\n","Validation - Epoch [1/20], Loss: 5.1757, Accuracy: 5.33%\n","\n","stop_count: 0\n","Training Epoch - [2/20], Batch [17/85], Loss: 5.1399\n","Training Epoch - [2/20], Batch [34/85], Loss: 5.1349\n","Training Epoch - [2/20], Batch [51/85], Loss: 5.0935\n","Training Epoch - [2/20], Batch [68/85], Loss: 5.0303\n","Training Epoch - [2/20], Batch [85/85], Loss: 4.9901\n","Epoch Loss: 5.0777\n","Validation - Epoch [2/20], Loss: 5.3717, Accuracy: 15.33%\n","\n","stop_count: 0\n","Training Epoch - [3/20], Batch [17/85], Loss: 4.8758\n","Training Epoch - [3/20], Batch [34/85], Loss: 4.7430\n","Training Epoch - [3/20], Batch [51/85], Loss: 4.7466\n","Training Epoch - [3/20], Batch [68/85], Loss: 4.7887\n","Training Epoch - [3/20], Batch [85/85], Loss: 4.7429\n","Epoch Loss: 4.7794\n","Validation - Epoch [3/20], Loss: 4.9592, Accuracy: 26.00%\n","\n","stop_count: 0\n","Training Epoch - [4/20], Batch [17/85], Loss: 4.5213\n","Training Epoch - [4/20], Batch [34/85], Loss: 4.4574\n","Training Epoch - [4/20], Batch [51/85], Loss: 4.3631\n","Training Epoch - [4/20], Batch [68/85], Loss: 4.4819\n","Training Epoch - [4/20], Batch [85/85], Loss: 4.4546\n","Epoch Loss: 4.4556\n","Validation - Epoch [4/20], Loss: 4.6270, Accuracy: 33.67%\n","\n","stop_count: 0\n","Training Epoch - [5/20], Batch [17/85], Loss: 4.0949\n","Training Epoch - [5/20], Batch [34/85], Loss: 4.1223\n","Training Epoch - [5/20], Batch [51/85], Loss: 4.1842\n","Training Epoch - [5/20], Batch [68/85], Loss: 4.0881\n","Training Epoch - [5/20], Batch [85/85], Loss: 4.1117\n","Epoch Loss: 4.1203\n","Validation - Epoch [5/20], Loss: 4.2721, Accuracy: 44.00%\n","\n","stop_count: 0\n","Training Epoch - [6/20], Batch [17/85], Loss: 4.0299\n","Training Epoch - [6/20], Batch [34/85], Loss: 3.9234\n","Training Epoch - [6/20], Batch [51/85], Loss: 4.0783\n","Training Epoch - [6/20], Batch [68/85], Loss: 4.0212\n","Training Epoch - [6/20], Batch [85/85], Loss: 3.9399\n","Epoch Loss: 3.9985\n","Validation - Epoch [6/20], Loss: 4.1326, Accuracy: 48.50%\n","\n","stop_count: 0\n","Training Epoch - [7/20], Batch [17/85], Loss: 3.8689\n","Training Epoch - [7/20], Batch [34/85], Loss: 3.8179\n","Training Epoch - [7/20], Batch [51/85], Loss: 3.8105\n","Training Epoch - [7/20], Batch [68/85], Loss: 3.7810\n","Training Epoch - [7/20], Batch [85/85], Loss: 3.7891\n","Epoch Loss: 3.8135\n","Validation - Epoch [7/20], Loss: 3.9409, Accuracy: 51.83%\n","\n","stop_count: 0\n","Training Epoch - [8/20], Batch [17/85], Loss: 3.8550\n","Training Epoch - [8/20], Batch [34/85], Loss: 3.7847\n","Training Epoch - [8/20], Batch [51/85], Loss: 3.6020\n","Training Epoch - [8/20], Batch [68/85], Loss: 3.6793\n","Training Epoch - [8/20], Batch [85/85], Loss: 3.5809\n","Epoch Loss: 3.7004\n","Validation - Epoch [8/20], Loss: 3.8445, Accuracy: 51.67%\n","\n","stop_count: 1\n","Training Epoch - [9/20], Batch [17/85], Loss: 3.7225\n","Training Epoch - [9/20], Batch [34/85], Loss: 3.6735\n","Training Epoch - [9/20], Batch [51/85], Loss: 3.5655\n","Training Epoch - [9/20], Batch [68/85], Loss: 3.7174\n","Training Epoch - [9/20], Batch [85/85], Loss: 3.5472\n","Epoch Loss: 3.6452\n","Validation - Epoch [9/20], Loss: 3.8088, Accuracy: 54.00%\n","\n","stop_count: 0\n","Training Epoch - [10/20], Batch [17/85], Loss: 3.6171\n","Training Epoch - [10/20], Batch [34/85], Loss: 3.6121\n","Training Epoch - [10/20], Batch [51/85], Loss: 3.3836\n","Training Epoch - [10/20], Batch [68/85], Loss: 3.6265\n","Training Epoch - [10/20], Batch [85/85], Loss: 3.5638\n","Epoch Loss: 3.5606\n","Validation - Epoch [10/20], Loss: 3.7978, Accuracy: 53.00%\n","\n","stop_count: 1\n","Training Epoch - [11/20], Batch [17/85], Loss: 3.4255\n","Training Epoch - [11/20], Batch [34/85], Loss: 3.5737\n","Training Epoch - [11/20], Batch [51/85], Loss: 3.4024\n","Training Epoch - [11/20], Batch [68/85], Loss: 3.4892\n","Training Epoch - [11/20], Batch [85/85], Loss: 3.4986\n","Epoch Loss: 3.4779\n","Validation - Epoch [11/20], Loss: 3.7300, Accuracy: 54.17%\n","\n","stop_count: 0\n","Training Epoch - [12/20], Batch [17/85], Loss: 3.6074\n","Training Epoch - [12/20], Batch [34/85], Loss: 3.4442\n","Training Epoch - [12/20], Batch [51/85], Loss: 3.4643\n","Training Epoch - [12/20], Batch [68/85], Loss: 3.4968\n","Training Epoch - [12/20], Batch [85/85], Loss: 3.5292\n","Epoch Loss: 3.5084\n","Validation - Epoch [12/20], Loss: 3.7020, Accuracy: 55.50%\n","\n","stop_count: 0\n","Training Epoch - [13/20], Batch [17/85], Loss: 3.5157\n","Training Epoch - [13/20], Batch [34/85], Loss: 3.4800\n","Training Epoch - [13/20], Batch [51/85], Loss: 3.3927\n","Training Epoch - [13/20], Batch [68/85], Loss: 3.3932\n","Training Epoch - [13/20], Batch [85/85], Loss: 3.4607\n","Epoch Loss: 3.4485\n","Validation - Epoch [13/20], Loss: 3.6549, Accuracy: 56.83%\n","\n","stop_count: 0\n","Training Epoch - [14/20], Batch [17/85], Loss: 3.5205\n","Training Epoch - [14/20], Batch [34/85], Loss: 3.3522\n","Training Epoch - [14/20], Batch [51/85], Loss: 3.3826\n","Training Epoch - [14/20], Batch [68/85], Loss: 3.3571\n","Training Epoch - [14/20], Batch [85/85], Loss: 3.3636\n","Epoch Loss: 3.3952\n","Validation - Epoch [14/20], Loss: 3.6480, Accuracy: 57.00%\n","\n","stop_count: 0\n","Training Epoch - [15/20], Batch [17/85], Loss: 3.3935\n","Training Epoch - [15/20], Batch [34/85], Loss: 3.3548\n","Training Epoch - [15/20], Batch [51/85], Loss: 3.4621\n","Training Epoch - [15/20], Batch [68/85], Loss: 3.1998\n","Training Epoch - [15/20], Batch [85/85], Loss: 3.3598\n","Epoch Loss: 3.3540\n","Validation - Epoch [15/20], Loss: 3.6070, Accuracy: 56.50%\n","\n","stop_count: 1\n","Training Epoch - [16/20], Batch [17/85], Loss: 3.3543\n","Training Epoch - [16/20], Batch [34/85], Loss: 3.3827\n","Training Epoch - [16/20], Batch [51/85], Loss: 3.3216\n","Training Epoch - [16/20], Batch [68/85], Loss: 3.4841\n","Training Epoch - [16/20], Batch [85/85], Loss: 3.4227\n","Epoch Loss: 3.3931\n","Validation - Epoch [16/20], Loss: 3.6204, Accuracy: 56.67%\n","\n","stop_count: 2\n","Training Epoch - [17/20], Batch [17/85], Loss: 3.2990\n","Training Epoch - [17/20], Batch [34/85], Loss: 3.4165\n","Training Epoch - [17/20], Batch [51/85], Loss: 3.3457\n","Training Epoch - [17/20], Batch [68/85], Loss: 3.2764\n","Training Epoch - [17/20], Batch [85/85], Loss: 3.3759\n","Epoch Loss: 3.3427\n","Validation - Epoch [17/20], Loss: 3.5909, Accuracy: 56.00%\n","\n","stop_count: 3\n","Training Epoch - [18/20], Batch [17/85], Loss: 3.4073\n","Training Epoch - [18/20], Batch [34/85], Loss: 3.2252\n","Training Epoch - [18/20], Batch [51/85], Loss: 3.5110\n","Training Epoch - [18/20], Batch [68/85], Loss: 3.4036\n","Training Epoch - [18/20], Batch [85/85], Loss: 3.3673\n","Epoch Loss: 3.3829\n","Validation - Epoch [18/20], Loss: 3.6026, Accuracy: 56.00%\n","\n","stop_count: 4\n","Testing Accuracy: 54.69%, Epochs: 18\n"]}],"source":["# Plots\n","TRAINING_LOSS = []\n","VALIDATION_LOSS = []\n","VALIDATION_ACCURACY = []\n","LEARNING_RATE = []\n","\n","f = open(\"/kaggle/working/Log.txt\", 'a')\n","f.truncate(0)\n","f.write(\"\\n\\n\\n\\n\\n\")\n","\n","# If GPU is available then use GPU else use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define data transforms\n","    # Resize images to 224x224\n","    # Normalize\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Dataset path\n","dataset = datasets.ImageFolder(root='/kaggle/input/cub-200-2011/CUB_200_2011/images', transform=transform)\n","\n","train_dataset, val_dataset, test_dataset = test_train_val_split(dataset)\n","\n","# Dataloaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Define EfficientNet-B2 model\n","model = EfficientNet.from_pretrained('efficientnet-b2')\n","# Set the number of classes to 200 \n","model._fc=torch.nn.Linear(in_features=model._fc.in_features, out_features=200)\n","# Freeze all the wights of the model\n","for name, param in model.named_parameters():\n","    param.requires_grad = False\n","# Un-freeze the last block, fully connected layer and batch normalization layers   \n","for name, param in model.named_parameters():\n","    if (name.split('.')[0] == \"_blocks\" and int(name.split('.')[1]) >= len(model._blocks) - 1) or (\"fc\" in name) or (\"bn\" in name) :  # Fix first 15 blocks\n","        param.requires_grad = True\n","# model._fc = nn.Sequential(\n","#     nn.Linear(model._fc.in_features, 200),  # Add a fully connected layer\n","#     nn.ReLU(inplace=True),  # Add activation function\n","#     nn.Dropout(0.2),  # Add dropout\n","#     )\n","# for name, param in model.named_parameters():\n","#     print(name)\n","\n","\n","best_model = copy.deepcopy(model)\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"\\nTotal number of parameters in the model: {total_params}\")\n","f.write(f\"\\nTotal number of parameters in the model: {total_params}\\n\")\n","\n","\n","trainable_parameters = []\n","for param in model.parameters():\n","    if param.requires_grad == True:\n","        trainable_parameters.append(param)\n","\n","number_of_trainable_parameters = sum(p.numel() for p in trainable_parameters)\n","print(f\"\\nTotal number of trainable parameters in the model: {number_of_trainable_parameters}\\n\")\n","f.write(f\"Total number of trainable parameters in the model: {number_of_trainable_parameters}\\n\\n\")\n","\n","\n","# Move the model to the GPU\n","model.to(device)\n","\n","# Define loss function, optimizer and learning rate scheduler\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = StepLR(optimizer, step_size=1,gamma=0.9)\n","\n","\n","print_every = 17\n","total_batches = len(train_loader)\n","max_val_accuracy = 0\n","stop_count = 0\n","running_loss = 0.0\n","epoch_loss = 0.0\n","val_loss = 0.0\n","num_epochs = 20\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    if(stop_count > 3 or epoch == num_epochs):\n","        break\n","\n","    model.train()\n","    for batch_idx, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == total_batches:\n","            avg_loss = running_loss / print_every\n","            print(f'Training Epoch - [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{total_batches}], Loss: {avg_loss:.4f}')\n","            f.write(f'Training Epoch - [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{total_batches}], Loss: {avg_loss:.4f}\\n')\n","            epoch_loss += running_loss\n","            running_loss = 0\n","    epoch_loss = epoch_loss / len(train_loader)\n","    TRAINING_LOSS.append(epoch_loss)\n","    \n","    print(f'Epoch Loss: {epoch_loss:.4f}')\n","    f.write(f'Epoch Loss: {epoch_loss:.4f}\\n')\n","    \n","    epoch_loss = 0\n","    \n","    # Validation loop \n","    model.eval()\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)  # Move data to CUDA if available\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            val_total += labels.size(0)\n","            val_correct += (predicted == labels).sum().item()\n","            \n","    val_loss = val_loss/len(val_loader)       \n","    VALIDATION_LOSS.append(val_loss)\n","    \n","    val_accuracy = 100 * val_correct / val_total\n","    VALIDATION_ACCURACY.append(val_accuracy)\n","    \n","    LEARNING_RATE.append(optimizer.param_groups[-1]['lr'])\n","    \n","    # Stoping criteria\n","    if(val_accuracy > max_val_accuracy):\n","        max_val_accuracy = val_accuracy\n","        torch.save(model.state_dict(), '/kaggle/working//efficientnet_b0_cub.pth')\n","        stop_count = 0\n","    else:\n","        stop_count += 1\n","    \n","    scheduler.step()\n","    \n","    print(f'Validation - Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n","    f.write(f'Validation - Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\\n')\n","    print(f\"\\nstop_count: {stop_count}\")\n","    \n","#Testing loop\n","best_model.load_state_dict(torch.load(\"/kaggle/working/efficientnet_b0_cub.pth\"))\n","best_model.to(device)\n","test(best_model, test_loader, device)\n","f.close()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4533155,"sourceId":7752919,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
